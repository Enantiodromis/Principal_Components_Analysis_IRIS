{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import sparse_encode\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset\n",
    "### Iris dataset contains:\n",
    "1. Iris.data that is an array where each row of which is a feature vector for one sample.\n",
    "2. Iris.target that is a vector defining the class labels associated with the corresponding rows of iris.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis\n",
    "The Iris dataset is represented by a four-dimensonal feature vector. It is difficult to visualise 4d data.\n",
    "One way to do it is to apply Principal Components Analysis to reduce the dimensionality of the data. \n",
    "\n",
    "## Performing PCA on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73979271,  0.65945457],\n",
       "       [ 1.87258591, -0.18119302],\n",
       "       [-0.54432857,  1.57186275],\n",
       "       [ 2.85697876, -0.14953001],\n",
       "       [ 0.83114667, -0.30612577]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_vectors = np.array([[7.2,2.8,4.4,0.3],\n",
    "                        [5.1,4.1,5.9,2.3],\n",
    "                        [7.9,3.0,2.5,0.6],\n",
    "                        [6.1,3.4,6.6,2.2],\n",
    "                        [6.1,2.6,4.7,0.9]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X,Y)\n",
    "pca.transform(pca_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparce Coding\n",
    "Using the \"Orthogonal Matching Pursuit\" (OMP) method. Impleneted by the \"sparse_encode\" function from the sklearn.decomposition library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coef_vectors = ([[6.3,2.3,2.6,2.0],\n",
    "                       [5.6,2.9,5.9,1.0],\n",
    "                       [7.6,3.9,4.5,1.3],\n",
    "                       [4.8,3.2,3.3,1.1],\n",
    "                       [5.9,2.4,6.1,2.3]])\n",
    "\n",
    "num_non_zero = 2\n",
    "tolerance = 1.000000e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a dictionary\n",
    "A class-specific dictionary containing all the samples from the Iris dataset that are in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = np.array([x for x, y_value in zip(X, Y) if y_value == 0])\n",
    "class_1_data = np.array([x for x, y_value in zip(X, Y) if y_value == 1])\n",
    "class_2_data = np.array([x for x, y_value in zip(X, Y) if y_value == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying sparse_encode with the newely defined dictionaries and defining the cost function\n",
    "The cost associated with each encoding is calculated using ||x−VTy||2+λ||y||0, where λ=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 costs for dictionaries for class 0, 1 and 2: [11.8655, 10.3788, 8.9039]\n",
      "Predicted class for sample 1 is: 2\n",
      "Sample 2 costs for dictionaries for class 0, 1 and 2: [13.3211, 10.9503, 11.3293]\n",
      "Predicted class for sample 2 is: 1\n",
      "Sample 3 costs for dictionaries for class 0, 1 and 2: [14.9361, 13.1153, 11.6346]\n",
      "Predicted class for sample 3 is: 2\n",
      "Sample 4 costs for dictionaries for class 0, 1 and 2: [9.228, 8.3104, 7.3432]\n",
      "Predicted class for sample 4 is: 2\n",
      "Sample 5 costs for dictionaries for class 0, 1 and 2: [13.2338, 10.6456, 11.0343]\n",
      "Predicted class for sample 5 is: 1\n"
     ]
    }
   ],
   "source": [
    "class_dictionaries = [class_0_data, class_1_data, class_2_data]\n",
    "\n",
    "for idx, sample in enumerate(sparse_coef_vectors, start=1):\n",
    "    costs = []\n",
    "    for class_dict in class_dictionaries:\n",
    "        class_pred = sparse_encode(\n",
    "            [sample],\n",
    "            class_dict, \n",
    "            algorithm='omp', \n",
    "            n_nonzero_coefs=num_non_zero, \n",
    "            alpha=tolerance\n",
    "        )\n",
    "        VT_y = class_dict.T.dot(class_pred.T) #Transposing and dot porduct of the two matrices\n",
    "        x_minus_VT_y = sample - VT_y\n",
    "        first_part = np.linalg.norm(x_minus_VT_y) #Euclidean distance calculation (Sqrt of sum of squares)\n",
    "        second_part = 0.1 * 2\n",
    "        \n",
    "        cost = np.round(first_part + second_part, 4)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "    print(f'Sample {idx} costs for dictionaries for class 0, 1 and 2: {costs}')\n",
    "    print(f'Predicted class for sample {idx} is: {np.argmin(costs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
