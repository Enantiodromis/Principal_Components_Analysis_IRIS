{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import sparse_encode\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the iris dataset\n",
    "### Iris dataset contains:\n",
    "1. Iris.data that is an array where each row of which is a feature vector for one sample.\n",
    "2. Iris.target that is a vector defining the class labels associated with the corresponding rows of iris.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis\n",
    "The Iris dataset is represented by a four-dimensonal feature vector. It is difficult to visualise 4d data.\n",
    "One way to do it is to apply Principal Components Analysis to reduce the dimensionality of the data. \n",
    "\n",
    "## Performing PCA on the Iris dataset:\n",
    "PCA computes vectors that you can project your data onto in order to reduce the dimension of your data. \n",
    "\n",
    "Since each row of your data is 4 dimensional there will be a maximum of 4 vectors onto which data can be projected and each of those vectors will be 2-dimensional. \n",
    "\n",
    "Each row of PCA.components_ is a single vector onto which things get projected, it will have the same size as the number of columns in your training data. \n",
    "\n",
    "Calling transform you're asking sklearn to actually do the projection. You are asking it to project each row of your data into the vector space that was learned when fit was called. For each row of the data you pass to transform you'll have 1 row in the output and the number of columns in that row will be the number of vectors that were learned in the fit phase. The number of columns will be equal to the value of n_components you passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7398,  0.6595],\n",
       "       [ 1.8726, -0.1812],\n",
       "       [-0.5443,  1.5719],\n",
       "       [ 2.857 , -0.1495],\n",
       "       [ 0.8311, -0.3061]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_vectors = np.array([[7.2,2.8,4.4,0.3],\n",
    "                        [5.1,4.1,5.9,2.3],\n",
    "                        [7.9,3.0,2.5,0.6],\n",
    "                        [6.1,3.4,6.6,2.2],\n",
    "                        [6.1,2.6,4.7,0.9]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X,Y)\n",
    "pca.transform(pca_vectors).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparce Coding\n",
    "Using the \"Orthogonal Matching Pursuit\" (OMP) method. Impleneted by the \"sparse_encode\" function from the sklearn.decomposition library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coef_vectors = ([[6.3,2.3,2.6,2.0],\n",
    "                       [5.6,2.9,5.9,1.0],\n",
    "                       [7.6,3.9,4.5,1.3],\n",
    "                       [4.8,3.2,3.3,1.1],\n",
    "                       [5.9,2.4,6.1,2.3]])\n",
    "\n",
    "num_non_zero = 2\n",
    "tolerance = 1.000000e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a dictionary\n",
    "A class-specific dictionary containing all the samples from the Iris dataset that are in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_data = np.array([x for x, y_value in zip(X, Y) if y_value == 0])\n",
    "class_1_data = np.array([x for x, y_value in zip(X, Y) if y_value == 1])\n",
    "class_2_data = np.array([x for x, y_value in zip(X, Y) if y_value == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying sparse_encode with the newely defined dictionaries and defining the cost function\n",
    "The cost associated with each encoding is calculated using ||x−VTy||2+λ||y||0, where λ=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 costs for dictionaries for class 0, 1 and 2: [1.9066, 1.5721, 1.4768]\n",
      "Predicted class for sample 1 is: 2\n",
      "Sample 2 costs for dictionaries for class 0, 1 and 2: [1.7357, 1.1069, 0.47]\n",
      "Predicted class for sample 2 is: 2\n",
      "Sample 3 costs for dictionaries for class 0, 1 and 2: [2.5009, 0.5616, 1.7803]\n",
      "Predicted class for sample 3 is: 1\n",
      "Sample 4 costs for dictionaries for class 0, 1 and 2: [1.3761, 0.7526, 0.736]\n",
      "Predicted class for sample 4 is: 2\n",
      "Sample 5 costs for dictionaries for class 0, 1 and 2: [3.006, 0.6005, 0.9553]\n",
      "Predicted class for sample 5 is: 1\n"
     ]
    }
   ],
   "source": [
    "class_dictionaries = [class_0_data, class_1_data, class_2_data]\n",
    "\n",
    "for idx, sample in enumerate(sparse_coef_vectors, start=1):\n",
    "    costs = []\n",
    "    for class_dict in class_dictionaries:\n",
    "        class_pred = sparse_encode(\n",
    "            [sample],\n",
    "            class_dict, \n",
    "            algorithm='omp', \n",
    "            n_nonzero_coefs=num_non_zero, \n",
    "            alpha=tolerance\n",
    "        )\n",
    "        VT_y = class_dict.T.dot(class_pred.T) #Transposing and dot porduct of the two matrices\n",
    "        x_minus_VT_y = sample - VT_y.T\n",
    "        first_part = np.linalg.norm(x_minus_VT_y) #Euclidean distance calculation (Sqrt of sum of squares)\n",
    "        second_part = 0.1 * 2\n",
    "        \n",
    "        cost = np.round(first_part + second_part, 4)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "    print(f'Sample {idx} costs for dictionaries for class 0, 1 and 2: {costs}')\n",
    "    print(f'Predicted class for sample {idx} is: {np.argmin(costs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}